#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict
import random
import operator



def initial_vec(n, m):
  x = range(len(n))
  vec = [random.choice(x) for y in range(len(m))]
  return vec

def initial_probs(n, m):
  x = len(n)
  vec = [[1.00000/x for y in range(len(m))] for z in range(len(n))]
  return vec

def choose_by_prob(probs, bitext):
  align = []
  #print probs
  for (n, (f, e)) in enumerate(bitext):
    current = []

    for e_j in e:

      max_score = 0
      max_pos = 0
      count = 0
      for f_i in f:

        if probs[f_i, e_j] > max_score:
          max_score = probs[f_i, e_j] 
          max_pos = count
        count += 1
      current.append(max_pos)
      
    align.append(current)

  return align


optparser = optparse.OptionParser()
optparser.add_option("-b", "--bitext", dest="bitext", default="data/dev-test-train.de-en", help="Parallel corpus (default data/dev-test-train.de-en)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()

sys.stderr.write("Training with IBM-1...")
bitext = [[sentence.strip().split() for sentence in pair.split(' ||| ')] for pair in open(opts.bitext)][:opts.num_sents]

#this randomly initializes the alignments
alignments = [initial_vec(x[0],x[1]) for x in bitext]



f_count = defaultdict(int)
fe_count = defaultdict(int)
e_count = defaultdict(int)
probs = defaultdict(float)


for (n, (f, e)) in enumerate(bitext):
  for j in range(len(e)):
    for i in range(len(f)):
      f_count[f[i]] = 1
      fe_count[f[i], e[j]] = 1
      e_count[e[j]] = 1
for (n, (f, e)) in enumerate(bitext):
  for j in range(len(e)):
    for i in range(len(f)):
      probs[f[i], e[j]] = 1.0/len(f_count)
for i in range(5):


  exp_count = defaultdict(int)
  total_count = defaultdict(int)
  norm = defaultdict(int)


  for (n, (f, e)) in enumerate(bitext):
    if n % 500 == 0:
      sys.stderr.write(".")

    for j in range(len(e)):
      norm[e[j]] = 0
      for i in range(len(f)):
        norm[e[j]] += probs[f[i], e[j]]

    for j in range(len(e)):
      for i in range(len(f)):
          exp_count[f[i], e[j]] += probs[f[i], e[j]]/norm[e[j]]
          total_count[f[i]] += probs[f[i], e[j]]/norm[e[j]]
  
  for (n, (f, e)) in enumerate(bitext):
    for i in f:
      for j in e:
        '''
        print 'e ',
        print len(e)
        print 'g ',
        print len(f)
        print 'probs ',
        print len(probs[n]),
        print ' ',
        print len(probs[n][0])
        '''
        probs[i, j] = exp_count[i, j]/total_count[i]

alignments = choose_by_prob(probs, bitext)
#print probs

#print alignments
for a in alignments:
  for i in range(len(a)):
    print "%i-%i " % (a[i],i),
  print ' '

'''
  for f_i in set(f):
    f_count[f_i] += 1
    for e_j in set(e):
      fe_count[(f_i,e_j)] += 1
  for e_j in set(e):
    e_count[e_j] += 1

  if n % 500 == 0:
    sys.stderr.write(".")

dice = defaultdict(int)
for (k, (f_i, e_j)) in enumerate(fe_count.keys()):
  dice[(f_i,e_j)] = 2.0 * fe_count[(f_i, e_j)] / (f_count[f_i] + e_count[e_j])
  if k % 5000 == 0:
    sys.stderr.write(".")
sys.stderr.write("\n")

for (f, e) in bitext:
  for (i, f_i) in enumerate(f): 
    for (j, e_j) in enumerate(e):
      if dice[(f_i,e_j)] >= opts.threshold:
        sys.stdout.write("%i-%i " % (i,j))
  sys.stdout.write("\n")'''
